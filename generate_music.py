import matplotlib
# matplotlib.use('Agg')
import os
from argparse import ArgumentParser
import scipy
import random

import numpy as np
import torch
import torch.nn as nn
from torch import optim
from torch.utils.data import TensorDataset, DataLoader
from torchvision.utils import save_image
from torch.autograd import Variable
from music21 import instrument, note, stream, chord

from c_rnn_gan import Generator, Discriminator
import utils
from config import parsers
import matplotlib.pyplot as plt

DATA_DIR_TRN = '../../DATA/nsdi_DATA'
# DATA_DIR_VAL = '../../DATA/DATA_2000_10/test'
ARGS = parsers()
G_FN = 'c_rnn_gan_g.pth'
D_FN = 'c_rnn_gan_d.pth'
gloss_array = []
dloss_array = []

MAX_GRAD_NORM = 5.0
BATCH_SIZE = 32
MAX_EPOCHS = 500
L2_DECAY = 1.0

MAX_SEQ_LEN = 300

PERFORM_LOSS_CHECKING = False
FREEZE_G = False
FREEZE_D = False

NUM_DUMMY_TRN = 256   # 训练数据集总共256
NUM_DUMMY_VAL = 128   # 验证数据集总共128

EPSILON = 1e-40 # value to use to approximate zero (to prevent undefined results)
cuda = True
FloatTensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor




def get_random_batches(path, batch_size):
    length = len([name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))]) - 1
    # index_train = np.random.choice(int(length),batch_size,  False)
    index_train = [0]
    data = []
    for index in index_train:
        tmp = np.load(path+'./{}.npy'.format(index))[:ARGS.seq_len]
        data.append(tmp)
    data = np.array(data)
    return data, np.load(path+'./{}.npy'.format(index))


def generate_sample(g_model, batches_done):
    ''' Sample from generator
    '''
    n_row = 5
    num_song = 1
    num_sample = n_row*n_row
    args = parsers()
    g_model.eval()
    z = torch.empty([num_song, args.latent_dim]).normal_(0, 1) # random vector
    labels, whole_song = get_random_batches(DATA_DIR_TRN, num_song)
    gen_labels = Variable(FloatTensor(labels)).squeeze()
    g_states = g_model.init_hidden(num_song)
    g_feats, _ = g_model(z, g_states, gen_labels)
    g_feats = g_feats.cpu().detach().numpy()

    notes_dict = np.load('./notes_dict.npy')
    max_value = len(notes_dict) - 1
    g_feats_final = np.array([int(np.round((x + 1) * 151).clip(0, max_value)) for x in g_feats])
    # save_image(g_feats.data, args.pics_path+ "%d.png" % batches_done, nrow=n_row, normalize=True)
    return np.concatenate((gen_labels.cpu().numpy().squeeze(), g_feats_final)), whole_song

def create_midi(prediction_output, name):
    """ convert the output from the prediction to notes and create a midi file
        from the notes """
    offset = 0
    output_notes = []
    notes_dict = np.load('./notes_dict.npy')
    max_value = len(notes_dict)-1
    # create note and chord objects based on the values generated by the model
    for pattern in prediction_output:
        # pattern is a chord
        pattern = notes_dict[int(pattern)]
        if ('.' in pattern) or pattern.isdigit():
            notes_in_chord = pattern.split('.')
            notes = []
            for current_note in notes_in_chord:
                new_note = note.Note(int(current_note))
                new_note.storedInstrument = instrument.Piano()
                notes.append(new_note)
            new_chord = chord.Chord(notes)
            new_chord.offset = offset
            output_notes.append(new_chord)
        # pattern is a note
        else:
            new_note = note.Note(pattern)
            new_note.offset = offset
            new_note.storedInstrument = instrument.Piano()
            output_notes.append(new_note)

        # increase offset each iteration so that notes do not stack
        offset += 0.5
    midi_stream = stream.Stream(output_notes)
    midi_stream.write('midi', fp='{}.mid'.format(name))

def main(args):
    ''' Training sequence
    '''
    trn_dataloader = utils.load_data(args.batch_size, DATA_DIR_TRN)
    val_dataloader = utils.load_data(args.batch_size, DATA_DIR_TRN)

    # First checking if GPU is available
    train_on_gpu = torch.cuda.is_available() and args.GPU
    if train_on_gpu:
        print('Training on GPU.')
    else:
        print('No GPU available, training on CPU.')

    model = {
        'g': Generator(num_feats=args.num_feats, use_cuda=train_on_gpu),
        'd': Discriminator(num_feats=args.num_feats, use_cuda=train_on_gpu)
    }


    generator_stat = torch.load('./Model2/g_epoch_1500.pt')['model_state_dict']
    model['g'].load_state_dict(generator_stat)
    model['g'].cuda()

    gen_whole_song, whole_song = generate_sample(model['g'], 0)
    plt.plot(gen_whole_song)
    plt.plot(whole_song)
    plt.show()
    create_midi(gen_whole_song, 'generated_song')
    create_midi(whole_song, 'real_song')

if __name__ == "__main__":


    MAX_SEQ_LEN = ARGS.seq_len
    BATCH_SIZE = ARGS.batch_size
    FREEZE_G = ARGS.freeze_g
    FREEZE_D = ARGS.freeze_d


    DATA_DIR_TRN = '../data'
    # DATA_DIR_VAL = '../../DATA/_DATA_{}_{}/test'.format(ARGS.time_length, ARGS.trace_sample_interval)
    print(ARGS)
    main(ARGS)
